{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7526a03b",
   "metadata": {},
   "source": [
    "# This notebook demonstrates performance of shallow vs. deep neural network. Deeper neural network would have better performance as the number of parameter required to learn increase\n",
    "Source: https://ojs.aaai.org/index.php/AAAI/article/view/10913"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3556556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# !pip install --upgrade --user h5py # I use this so solve the tensorflow problem\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Dense, Activation\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3850e283",
   "metadata": {},
   "source": [
    "## Generate sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e1877ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# generate 120K random samples\n",
    "uniform_samples = np.random.uniform(-2 * np.pi, 2 * np.pi, 120000)\n",
    "\n",
    "# randomly select 60K as training and 60K as testing\n",
    "X_train = np.array(np.random.choice(uniform_samples, size = 60000, replace = False))\n",
    "X_test = np.array(np.setdiff1d(uniform_samples, train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "22a84689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the dimension of the data\n",
    "# X_train.reshape((60000, 1))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b3d94",
   "metadata": {},
   "source": [
    "## Define the sparse trigonometric polynomial function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fdb7856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return 2 * (2 * (np.cos(x) ** 2) - 1) ** 2 - 1\n",
    "\n",
    "# generate y through func()\n",
    "y_train = func(X_train)\n",
    "y_test = func(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac6cd12",
   "metadata": {},
   "source": [
    "## Build the model structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "33c8f7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 hidden layer\n",
    "\n",
    "def creat_model_1_layer(units_item):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    # hidden layer 1\n",
    "    model.add(Dense(units = units_item, input_shape=(1,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(rate = 0.25))\n",
    "\n",
    "    # output layer (unit = 1 for regression)\n",
    "    model.add(Dense(units = 1))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    # compile\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate = 0.0001),\n",
    "                  loss=\"mse\",\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9483d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 hidden layers\n",
    "\n",
    "def creat_model_2_layer(units_item):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    # hidden layer 1\n",
    "    model.add(Dense(units = units_item, input_shape=(1,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(rate = 0.25))\n",
    "\n",
    "    # hidden layer 2\n",
    "    model.add(Dense(units = units_item))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(rate = 0.25))\n",
    "\n",
    "    # output layer (unit = 1 for regression)\n",
    "    model.add(Dense(units = 1))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    # compile\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate = 0.0001),\n",
    "                  loss=\"mse\",\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d226bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 hidden layers\n",
    "\n",
    "def creat_model_3_layer(units_item):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    # hidden layer 1\n",
    "    model.add(Dense(units = units_item, input_shape=(1,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(rate = 0.25))\n",
    "    \n",
    "    # hidden layer 2\n",
    "    model.add(Dense(units = units_item))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(rate = 0.25))\n",
    "    \n",
    "    # hidden layer 3\n",
    "    model.add(Dense(units = units_item))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(rate = 0.25))\n",
    "\n",
    "    # output layer (unit = 1 for regression)\n",
    "    model.add(Dense(units = 1))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    # compile\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate = 0.0001),\n",
    "                  loss=\"mse\",\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2fbf78",
   "metadata": {},
   "source": [
    "## Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ac034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the units of neurons I want to try\n",
    "units_list = [20, 40, 60, 80, 100, 120, 140, 160, 180, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7b9c41bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "16/16 [==============================] - 3s 53ms/step - loss: 0.5331 - root_mean_squared_error: 0.7302 - val_loss: 0.4930 - val_root_mean_squared_error: 0.7021\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.5304 - root_mean_squared_error: 0.7283 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.5279 - root_mean_squared_error: 0.7266 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.5236 - root_mean_squared_error: 0.7236 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.5242 - root_mean_squared_error: 0.7240 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.5209 - root_mean_squared_error: 0.7217 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.5186 - root_mean_squared_error: 0.7201 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.5172 - root_mean_squared_error: 0.7192 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.5173 - root_mean_squared_error: 0.7192 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.5146 - root_mean_squared_error: 0.7173 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.5133 - root_mean_squared_error: 0.7164 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.5126 - root_mean_squared_error: 0.7160 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.5110 - root_mean_squared_error: 0.7148 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.5112 - root_mean_squared_error: 0.7150 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.5110 - root_mean_squared_error: 0.7149 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.5094 - root_mean_squared_error: 0.7137 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.5101 - root_mean_squared_error: 0.7142 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.5096 - root_mean_squared_error: 0.7138 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.5079 - root_mean_squared_error: 0.7127 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.5066 - root_mean_squared_error: 0.7118 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.5052 - root_mean_squared_error: 0.7108 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5061 - root_mean_squared_error: 0.7114 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.5040 - root_mean_squared_error: 0.7099 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.5042 - root_mean_squared_error: 0.7101 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.5041 - root_mean_squared_error: 0.7100 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.5026 - root_mean_squared_error: 0.7089 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.5040 - root_mean_squared_error: 0.7099 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.5032 - root_mean_squared_error: 0.7094 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.5023 - root_mean_squared_error: 0.7088 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.5040 - root_mean_squared_error: 0.7099 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_44 (Dense)            (None, 20)                40        \n",
      "                                                                 \n",
      " batch_normalization_21 (Ba  (None, 20)                80        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_41 (Activation)  (None, 20)                0         \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 20)                0         \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      " activation_42 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 141 (564.00 Byte)\n",
      "Trainable params: 101 (404.00 Byte)\n",
      "Non-trainable params: 40 (160.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 3s 44ms/step - loss: 3.0491 - root_mean_squared_error: 1.7462 - val_loss: 1.4422 - val_root_mean_squared_error: 1.2009\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.9302 - root_mean_squared_error: 1.7118 - val_loss: 1.3857 - val_root_mean_squared_error: 1.1772\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 2.8053 - root_mean_squared_error: 1.6749 - val_loss: 1.3341 - val_root_mean_squared_error: 1.1550\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 2.7235 - root_mean_squared_error: 1.6503 - val_loss: 1.2858 - val_root_mean_squared_error: 1.1339\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 2.5671 - root_mean_squared_error: 1.6022 - val_loss: 1.2402 - val_root_mean_squared_error: 1.1136\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 2.4873 - root_mean_squared_error: 1.5771 - val_loss: 1.1956 - val_root_mean_squared_error: 1.0934\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 2.3921 - root_mean_squared_error: 1.5466 - val_loss: 1.1518 - val_root_mean_squared_error: 1.0732\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.2749 - root_mean_squared_error: 1.5083 - val_loss: 1.1082 - val_root_mean_squared_error: 1.0527\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 2.1717 - root_mean_squared_error: 1.4737 - val_loss: 1.0648 - val_root_mean_squared_error: 1.0319\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 2.0577 - root_mean_squared_error: 1.4345 - val_loss: 1.0211 - val_root_mean_squared_error: 1.0105\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.9598 - root_mean_squared_error: 1.3999 - val_loss: 0.9765 - val_root_mean_squared_error: 0.9882\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.8101 - root_mean_squared_error: 1.3454 - val_loss: 0.9337 - val_root_mean_squared_error: 0.9663\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.6765 - root_mean_squared_error: 1.2948 - val_loss: 0.8918 - val_root_mean_squared_error: 0.9443\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.4987 - root_mean_squared_error: 1.2242 - val_loss: 0.8567 - val_root_mean_squared_error: 0.9256\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 1.4279 - root_mean_squared_error: 1.1950 - val_loss: 0.8270 - val_root_mean_squared_error: 0.9094\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.3774 - root_mean_squared_error: 1.1736 - val_loss: 0.7967 - val_root_mean_squared_error: 0.8926\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.3258 - root_mean_squared_error: 1.1514 - val_loss: 0.7654 - val_root_mean_squared_error: 0.8749\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.2976 - root_mean_squared_error: 1.1391 - val_loss: 0.7332 - val_root_mean_squared_error: 0.8563\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.2521 - root_mean_squared_error: 1.1190 - val_loss: 0.7002 - val_root_mean_squared_error: 0.8368\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.2065 - root_mean_squared_error: 1.0984 - val_loss: 0.6666 - val_root_mean_squared_error: 0.8165\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.1620 - root_mean_squared_error: 1.0780 - val_loss: 0.6328 - val_root_mean_squared_error: 0.7955\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.1285 - root_mean_squared_error: 1.0623 - val_loss: 0.5985 - val_root_mean_squared_error: 0.7736\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.0729 - root_mean_squared_error: 1.0358 - val_loss: 0.5654 - val_root_mean_squared_error: 0.7519\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.9860 - root_mean_squared_error: 0.9930 - val_loss: 0.5357 - val_root_mean_squared_error: 0.7319\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.8270 - root_mean_squared_error: 0.9094 - val_loss: 0.5112 - val_root_mean_squared_error: 0.7150\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.6799 - root_mean_squared_error: 0.8246 - val_loss: 0.5054 - val_root_mean_squared_error: 0.7109\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.6655 - root_mean_squared_error: 0.8158 - val_loss: 0.5031 - val_root_mean_squared_error: 0.7093\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.6632 - root_mean_squared_error: 0.8144 - val_loss: 0.5010 - val_root_mean_squared_error: 0.7078\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6566 - root_mean_squared_error: 0.8103 - val_loss: 0.4992 - val_root_mean_squared_error: 0.7065\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.6490 - root_mean_squared_error: 0.8056 - val_loss: 0.4977 - val_root_mean_squared_error: 0.7055\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5074 - root_mean_squared_error: 0.7123\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_46 (Dense)            (None, 40)                80        \n",
      "                                                                 \n",
      " batch_normalization_22 (Ba  (None, 40)                160       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_43 (Activation)  (None, 40)                0         \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 40)                0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 1)                 41        \n",
      "                                                                 \n",
      " activation_44 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 281 (1.10 KB)\n",
      "Trainable params: 201 (804.00 Byte)\n",
      "Non-trainable params: 80 (320.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 4s 57ms/step - loss: 0.8642 - root_mean_squared_error: 0.9296 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.7874 - root_mean_squared_error: 0.8874 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.7288 - root_mean_squared_error: 0.8537 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6807 - root_mean_squared_error: 0.8251 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6378 - root_mean_squared_error: 0.7987 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.5986 - root_mean_squared_error: 0.7737 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.5825 - root_mean_squared_error: 0.7632 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.5638 - root_mean_squared_error: 0.7508 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.5481 - root_mean_squared_error: 0.7404 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.5344 - root_mean_squared_error: 0.7310 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.5234 - root_mean_squared_error: 0.7235 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.5198 - root_mean_squared_error: 0.7210 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.5199 - root_mean_squared_error: 0.7210 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.5179 - root_mean_squared_error: 0.7196 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.5165 - root_mean_squared_error: 0.7187 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.5161 - root_mean_squared_error: 0.7184 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.5169 - root_mean_squared_error: 0.7189 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.5144 - root_mean_squared_error: 0.7172 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.5134 - root_mean_squared_error: 0.7165 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.5092 - root_mean_squared_error: 0.7136 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.5070 - root_mean_squared_error: 0.7121 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.5035 - root_mean_squared_error: 0.7096 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.5027 - root_mean_squared_error: 0.7090 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.5030 - root_mean_squared_error: 0.7092 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.5029 - root_mean_squared_error: 0.7092 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.5022 - root_mean_squared_error: 0.7087 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.5025 - root_mean_squared_error: 0.7089 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.5022 - root_mean_squared_error: 0.7086 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.5028 - root_mean_squared_error: 0.7091 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.5021 - root_mean_squared_error: 0.7086 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_48 (Dense)            (None, 60)                120       \n",
      "                                                                 \n",
      " batch_normalization_23 (Ba  (None, 60)                240       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_45 (Activation)  (None, 60)                0         \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 60)                0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 1)                 61        \n",
      "                                                                 \n",
      " activation_46 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 421 (1.64 KB)\n",
      "Trainable params: 301 (1.18 KB)\n",
      "Non-trainable params: 120 (480.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 3s 42ms/step - loss: 0.6374 - root_mean_squared_error: 0.7984 - val_loss: 0.4988 - val_root_mean_squared_error: 0.7063\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.6063 - root_mean_squared_error: 0.7786 - val_loss: 0.4929 - val_root_mean_squared_error: 0.7020\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.5791 - root_mean_squared_error: 0.7610 - val_loss: 0.4919 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.5608 - root_mean_squared_error: 0.7488 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.5465 - root_mean_squared_error: 0.7392 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.5339 - root_mean_squared_error: 0.7307 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.5287 - root_mean_squared_error: 0.7271 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.5239 - root_mean_squared_error: 0.7238 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.5200 - root_mean_squared_error: 0.7211 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.5156 - root_mean_squared_error: 0.7180 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.5143 - root_mean_squared_error: 0.7172 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.5144 - root_mean_squared_error: 0.7172 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.5134 - root_mean_squared_error: 0.7165 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.5110 - root_mean_squared_error: 0.7149 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.5111 - root_mean_squared_error: 0.7149 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.5080 - root_mean_squared_error: 0.7128 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.5095 - root_mean_squared_error: 0.7138 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.5070 - root_mean_squared_error: 0.7120 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.5073 - root_mean_squared_error: 0.7122 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.5074 - root_mean_squared_error: 0.7123 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.5065 - root_mean_squared_error: 0.7117 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.5071 - root_mean_squared_error: 0.7121 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.5048 - root_mean_squared_error: 0.7105 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.5060 - root_mean_squared_error: 0.7113 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.5049 - root_mean_squared_error: 0.7106 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.5036 - root_mean_squared_error: 0.7097 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.5037 - root_mean_squared_error: 0.7097 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.5038 - root_mean_squared_error: 0.7098 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.5037 - root_mean_squared_error: 0.7097 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.5038 - root_mean_squared_error: 0.7098 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_50 (Dense)            (None, 80)                160       \n",
      "                                                                 \n",
      " batch_normalization_24 (Ba  (None, 80)                320       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_47 (Activation)  (None, 80)                0         \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 80)                0         \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 1)                 81        \n",
      "                                                                 \n",
      " activation_48 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 561 (2.19 KB)\n",
      "Trainable params: 401 (1.57 KB)\n",
      "Non-trainable params: 160 (640.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 3s 54ms/step - loss: 1.0841 - root_mean_squared_error: 1.0412 - val_loss: 0.6911 - val_root_mean_squared_error: 0.8314\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.9401 - root_mean_squared_error: 0.9696 - val_loss: 0.6422 - val_root_mean_squared_error: 0.8014\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.8389 - root_mean_squared_error: 0.9159 - val_loss: 0.6035 - val_root_mean_squared_error: 0.7769\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.7632 - root_mean_squared_error: 0.8736 - val_loss: 0.5721 - val_root_mean_squared_error: 0.7564\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.7127 - root_mean_squared_error: 0.8442 - val_loss: 0.5458 - val_root_mean_squared_error: 0.7388\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.6689 - root_mean_squared_error: 0.8179 - val_loss: 0.5253 - val_root_mean_squared_error: 0.7248\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.6359 - root_mean_squared_error: 0.7974 - val_loss: 0.5108 - val_root_mean_squared_error: 0.7147\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.6072 - root_mean_squared_error: 0.7792 - val_loss: 0.5013 - val_root_mean_squared_error: 0.7080\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.5899 - root_mean_squared_error: 0.7680 - val_loss: 0.4946 - val_root_mean_squared_error: 0.7033\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.5805 - root_mean_squared_error: 0.7619 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7015\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.5624 - root_mean_squared_error: 0.7500 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.5565 - root_mean_squared_error: 0.7460 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.5518 - root_mean_squared_error: 0.7428 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.5477 - root_mean_squared_error: 0.7400 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.5407 - root_mean_squared_error: 0.7353 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.5380 - root_mean_squared_error: 0.7335 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.5323 - root_mean_squared_error: 0.7296 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.5315 - root_mean_squared_error: 0.7291 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.5298 - root_mean_squared_error: 0.7279 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.5254 - root_mean_squared_error: 0.7248 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.5253 - root_mean_squared_error: 0.7248 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.5210 - root_mean_squared_error: 0.7218 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.5206 - root_mean_squared_error: 0.7215 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.5180 - root_mean_squared_error: 0.7197 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.5171 - root_mean_squared_error: 0.7191 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.5165 - root_mean_squared_error: 0.7187 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.5150 - root_mean_squared_error: 0.7176 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.5142 - root_mean_squared_error: 0.7171 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.5156 - root_mean_squared_error: 0.7180 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.5128 - root_mean_squared_error: 0.7161 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_52 (Dense)            (None, 100)               200       \n",
      "                                                                 \n",
      " batch_normalization_25 (Ba  (None, 100)               400       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_49 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      " activation_50 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 701 (2.74 KB)\n",
      "Trainable params: 501 (1.96 KB)\n",
      "Non-trainable params: 200 (800.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 4s 74ms/step - loss: 0.6807 - root_mean_squared_error: 0.8250 - val_loss: 0.5729 - val_root_mean_squared_error: 0.7569\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.6173 - root_mean_squared_error: 0.7857 - val_loss: 0.5385 - val_root_mean_squared_error: 0.7338\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.5759 - root_mean_squared_error: 0.7589 - val_loss: 0.5146 - val_root_mean_squared_error: 0.7174\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.5527 - root_mean_squared_error: 0.7434 - val_loss: 0.5002 - val_root_mean_squared_error: 0.7073\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.5358 - root_mean_squared_error: 0.7320 - val_loss: 0.4939 - val_root_mean_squared_error: 0.7028\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.5302 - root_mean_squared_error: 0.7281 - val_loss: 0.4917 - val_root_mean_squared_error: 0.7012\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.5206 - root_mean_squared_error: 0.7215 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.5165 - root_mean_squared_error: 0.7187 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.5113 - root_mean_squared_error: 0.7150 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.5085 - root_mean_squared_error: 0.7131 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.5070 - root_mean_squared_error: 0.7121 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.5072 - root_mean_squared_error: 0.7122 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.5054 - root_mean_squared_error: 0.7109 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.5045 - root_mean_squared_error: 0.7103 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.5045 - root_mean_squared_error: 0.7103 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.5039 - root_mean_squared_error: 0.7098 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.5038 - root_mean_squared_error: 0.7098 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 1s 30ms/step - loss: 0.5039 - root_mean_squared_error: 0.7099 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.5033 - root_mean_squared_error: 0.7094 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.5025 - root_mean_squared_error: 0.7089 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.5024 - root_mean_squared_error: 0.7088 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.5031 - root_mean_squared_error: 0.7093 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.5023 - root_mean_squared_error: 0.7088 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.5026 - root_mean_squared_error: 0.7090 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.5028 - root_mean_squared_error: 0.7091 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.5023 - root_mean_squared_error: 0.7087 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.5018 - root_mean_squared_error: 0.7084 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.5020 - root_mean_squared_error: 0.7085 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.5016 - root_mean_squared_error: 0.7083 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.5016 - root_mean_squared_error: 0.7083 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_54 (Dense)            (None, 120)               240       \n",
      "                                                                 \n",
      " batch_normalization_26 (Ba  (None, 120)               480       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_51 (Activation)  (None, 120)               0         \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 1)                 121       \n",
      "                                                                 \n",
      " activation_52 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 841 (3.29 KB)\n",
      "Trainable params: 601 (2.35 KB)\n",
      "Non-trainable params: 240 (960.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 4s 65ms/step - loss: 1.0992 - root_mean_squared_error: 1.0484 - val_loss: 0.6122 - val_root_mean_squared_error: 0.7825\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.9299 - root_mean_squared_error: 0.9643 - val_loss: 0.5775 - val_root_mean_squared_error: 0.7600\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.8252 - root_mean_squared_error: 0.9084 - val_loss: 0.5483 - val_root_mean_squared_error: 0.7405\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.7392 - root_mean_squared_error: 0.8598 - val_loss: 0.5256 - val_root_mean_squared_error: 0.7250\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.6913 - root_mean_squared_error: 0.8315 - val_loss: 0.5092 - val_root_mean_squared_error: 0.7136\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.6449 - root_mean_squared_error: 0.8030 - val_loss: 0.4994 - val_root_mean_squared_error: 0.7067\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.6092 - root_mean_squared_error: 0.7805 - val_loss: 0.4933 - val_root_mean_squared_error: 0.7023\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.5919 - root_mean_squared_error: 0.7694 - val_loss: 0.4919 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.5677 - root_mean_squared_error: 0.7534 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.5605 - root_mean_squared_error: 0.7487 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.5495 - root_mean_squared_error: 0.7413 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.5416 - root_mean_squared_error: 0.7359 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.5351 - root_mean_squared_error: 0.7315 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.5289 - root_mean_squared_error: 0.7273 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.5219 - root_mean_squared_error: 0.7224 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.5171 - root_mean_squared_error: 0.7191 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.5151 - root_mean_squared_error: 0.7177 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.5122 - root_mean_squared_error: 0.7157 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.5098 - root_mean_squared_error: 0.7140 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.5112 - root_mean_squared_error: 0.7150 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.5099 - root_mean_squared_error: 0.7141 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.5085 - root_mean_squared_error: 0.7131 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.5071 - root_mean_squared_error: 0.7121 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.5062 - root_mean_squared_error: 0.7115 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.5063 - root_mean_squared_error: 0.7116 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.5042 - root_mean_squared_error: 0.7101 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.5052 - root_mean_squared_error: 0.7108 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.5035 - root_mean_squared_error: 0.7096 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.5039 - root_mean_squared_error: 0.7099 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.5039 - root_mean_squared_error: 0.7098 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_56 (Dense)            (None, 140)               280       \n",
      "                                                                 \n",
      " batch_normalization_27 (Ba  (None, 140)               560       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_53 (Activation)  (None, 140)               0         \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 140)               0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 1)                 141       \n",
      "                                                                 \n",
      " activation_54 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 981 (3.83 KB)\n",
      "Trainable params: 701 (2.74 KB)\n",
      "Non-trainable params: 280 (1.09 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 3s 62ms/step - loss: 0.5351 - root_mean_squared_error: 0.7315 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.5215 - root_mean_squared_error: 0.7221 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.5125 - root_mean_squared_error: 0.7159 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.5089 - root_mean_squared_error: 0.7134 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.5060 - root_mean_squared_error: 0.7113 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.5044 - root_mean_squared_error: 0.7102 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.5038 - root_mean_squared_error: 0.7098 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 0.5039 - root_mean_squared_error: 0.7098 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 0.5031 - root_mean_squared_error: 0.7093 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 1s 44ms/step - loss: 0.5024 - root_mean_squared_error: 0.7088 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.5026 - root_mean_squared_error: 0.7090 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.5019 - root_mean_squared_error: 0.7084 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 1s 42ms/step - loss: 0.5016 - root_mean_squared_error: 0.7083 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.5014 - root_mean_squared_error: 0.7081 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 1s 42ms/step - loss: 0.5017 - root_mean_squared_error: 0.7083 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 1s 42ms/step - loss: 0.5014 - root_mean_squared_error: 0.7081 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.5013 - root_mean_squared_error: 0.7080 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 0.5011 - root_mean_squared_error: 0.7079 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 0.5014 - root_mean_squared_error: 0.7081 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 1s 42ms/step - loss: 0.5012 - root_mean_squared_error: 0.7079 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.5010 - root_mean_squared_error: 0.7078 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.5008 - root_mean_squared_error: 0.7077 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.5008 - root_mean_squared_error: 0.7077 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.5008 - root_mean_squared_error: 0.7077 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.5010 - root_mean_squared_error: 0.7078 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 0.5008 - root_mean_squared_error: 0.7077 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.5006 - root_mean_squared_error: 0.7076 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.5009 - root_mean_squared_error: 0.7078 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.5009 - root_mean_squared_error: 0.7078 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_58 (Dense)            (None, 160)               320       \n",
      "                                                                 \n",
      " batch_normalization_28 (Ba  (None, 160)               640       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_55 (Activation)  (None, 160)               0         \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 160)               0         \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 1)                 161       \n",
      "                                                                 \n",
      " activation_56 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1121 (4.38 KB)\n",
      "Trainable params: 801 (3.13 KB)\n",
      "Non-trainable params: 320 (1.25 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 4s 73ms/step - loss: 0.7558 - root_mean_squared_error: 0.8694 - val_loss: 0.5044 - val_root_mean_squared_error: 0.7102\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.6591 - root_mean_squared_error: 0.8118 - val_loss: 0.4933 - val_root_mean_squared_error: 0.7023\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.6051 - root_mean_squared_error: 0.7779 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.5629 - root_mean_squared_error: 0.7502 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.5409 - root_mean_squared_error: 0.7355 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.5315 - root_mean_squared_error: 0.7290 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.5195 - root_mean_squared_error: 0.7208 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 1s 43ms/step - loss: 0.5172 - root_mean_squared_error: 0.7192 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.5134 - root_mean_squared_error: 0.7165 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.5106 - root_mean_squared_error: 0.7145 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.5076 - root_mean_squared_error: 0.7125 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.5055 - root_mean_squared_error: 0.7110 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.5049 - root_mean_squared_error: 0.7106 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.5052 - root_mean_squared_error: 0.7107 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 1s 42ms/step - loss: 0.5048 - root_mean_squared_error: 0.7105 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.5040 - root_mean_squared_error: 0.7099 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.5033 - root_mean_squared_error: 0.7094 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.5026 - root_mean_squared_error: 0.7090 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.5035 - root_mean_squared_error: 0.7096 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.5025 - root_mean_squared_error: 0.7089 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.5029 - root_mean_squared_error: 0.7091 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.5026 - root_mean_squared_error: 0.7090 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 0.5024 - root_mean_squared_error: 0.7088 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.5025 - root_mean_squared_error: 0.7089 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.5019 - root_mean_squared_error: 0.7085 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.5019 - root_mean_squared_error: 0.7084 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 1s 43ms/step - loss: 0.5022 - root_mean_squared_error: 0.7087 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.5015 - root_mean_squared_error: 0.7081 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.5016 - root_mean_squared_error: 0.7083 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.5018 - root_mean_squared_error: 0.7084 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_60 (Dense)            (None, 180)               360       \n",
      "                                                                 \n",
      " batch_normalization_29 (Ba  (None, 180)               720       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_57 (Activation)  (None, 180)               0         \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 180)               0         \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 1)                 181       \n",
      "                                                                 \n",
      " activation_58 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1261 (4.93 KB)\n",
      "Trainable params: 901 (3.52 KB)\n",
      "Non-trainable params: 360 (1.41 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 4s 66ms/step - loss: 0.8469 - root_mean_squared_error: 0.9203 - val_loss: 0.4961 - val_root_mean_squared_error: 0.7043\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.7018 - root_mean_squared_error: 0.8377 - val_loss: 0.4919 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.6281 - root_mean_squared_error: 0.7926 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 1s 43ms/step - loss: 0.5866 - root_mean_squared_error: 0.7659 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 1s 44ms/step - loss: 0.5563 - root_mean_squared_error: 0.7459 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.5390 - root_mean_squared_error: 0.7342 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.5294 - root_mean_squared_error: 0.7276 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 0.5209 - root_mean_squared_error: 0.7217 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 0.5162 - root_mean_squared_error: 0.7185 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 0.5114 - root_mean_squared_error: 0.7151 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 0.5090 - root_mean_squared_error: 0.7135 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.5083 - root_mean_squared_error: 0.7130 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.5061 - root_mean_squared_error: 0.7114 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 0.5065 - root_mean_squared_error: 0.7117 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.5053 - root_mean_squared_error: 0.7109 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 0.5040 - root_mean_squared_error: 0.7099 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.5036 - root_mean_squared_error: 0.7097 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.5036 - root_mean_squared_error: 0.7097 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 0.5032 - root_mean_squared_error: 0.7094 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.5027 - root_mean_squared_error: 0.7090 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 1s 44ms/step - loss: 0.5023 - root_mean_squared_error: 0.7087 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 0.5028 - root_mean_squared_error: 0.7091 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 0.5029 - root_mean_squared_error: 0.7091 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 1s 44ms/step - loss: 0.5024 - root_mean_squared_error: 0.7088 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.5020 - root_mean_squared_error: 0.7085 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 1s 44ms/step - loss: 0.5019 - root_mean_squared_error: 0.7085 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 0.5019 - root_mean_squared_error: 0.7084 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.5013 - root_mean_squared_error: 0.7081 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 1s 42ms/step - loss: 0.5021 - root_mean_squared_error: 0.7086 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.5015 - root_mean_squared_error: 0.7081 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_62 (Dense)            (None, 200)               400       \n",
      "                                                                 \n",
      " batch_normalization_30 (Ba  (None, 200)               800       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_59 (Activation)  (None, 200)               0         \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 1)                 201       \n",
      "                                                                 \n",
      " activation_60 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1401 (5.47 KB)\n",
      "Trainable params: 1001 (3.91 KB)\n",
      "Non-trainable params: 400 (1.56 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1 hidden layer\n",
    "\n",
    "rmse_1 = []\n",
    "\n",
    "for i, value in enumerate(units_list):\n",
    "    \n",
    "    # train the nn model on training set\n",
    "    model = creat_model_1_layer(units_list[i]) \n",
    "    model.fit(X_train, y_train, batch_size = 3000, epochs=30, validation_split=0.2)\n",
    "    \n",
    "    # get prediction from testing set\n",
    "    score = model.evaluate(X_test, y_test)\n",
    "    rmse_1.append(score[1]) # score[0] is the loss\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f649dc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "16/16 [==============================] - 5s 58ms/step - loss: 0.9856 - root_mean_squared_error: 0.9928 - val_loss: 0.5508 - val_root_mean_squared_error: 0.7422\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.9442 - root_mean_squared_error: 0.9717 - val_loss: 0.5284 - val_root_mean_squared_error: 0.7269\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.9257 - root_mean_squared_error: 0.9621 - val_loss: 0.5114 - val_root_mean_squared_error: 0.7152\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.8823 - root_mean_squared_error: 0.9393 - val_loss: 0.5013 - val_root_mean_squared_error: 0.7081\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.8685 - root_mean_squared_error: 0.9320 - val_loss: 0.4975 - val_root_mean_squared_error: 0.7054\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.8453 - root_mean_squared_error: 0.9194 - val_loss: 0.4994 - val_root_mean_squared_error: 0.7067\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.8152 - root_mean_squared_error: 0.9029 - val_loss: 0.5005 - val_root_mean_squared_error: 0.7074\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.7930 - root_mean_squared_error: 0.8905 - val_loss: 0.5003 - val_root_mean_squared_error: 0.7073\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.7773 - root_mean_squared_error: 0.8817 - val_loss: 0.4994 - val_root_mean_squared_error: 0.7067\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.7569 - root_mean_squared_error: 0.8700 - val_loss: 0.4982 - val_root_mean_squared_error: 0.7058\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.7417 - root_mean_squared_error: 0.8612 - val_loss: 0.4967 - val_root_mean_squared_error: 0.7048\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.7317 - root_mean_squared_error: 0.8554 - val_loss: 0.4952 - val_root_mean_squared_error: 0.7037\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7152 - root_mean_squared_error: 0.8457 - val_loss: 0.4942 - val_root_mean_squared_error: 0.7030\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.7006 - root_mean_squared_error: 0.8370 - val_loss: 0.4934 - val_root_mean_squared_error: 0.7024\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6826 - root_mean_squared_error: 0.8262 - val_loss: 0.4925 - val_root_mean_squared_error: 0.7018\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6824 - root_mean_squared_error: 0.8260 - val_loss: 0.4914 - val_root_mean_squared_error: 0.7010\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6699 - root_mean_squared_error: 0.8185 - val_loss: 0.4906 - val_root_mean_squared_error: 0.7004\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.6609 - root_mean_squared_error: 0.8130 - val_loss: 0.4901 - val_root_mean_squared_error: 0.7001\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.6491 - root_mean_squared_error: 0.8057 - val_loss: 0.4901 - val_root_mean_squared_error: 0.7000\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.6365 - root_mean_squared_error: 0.7978 - val_loss: 0.4897 - val_root_mean_squared_error: 0.6998\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6291 - root_mean_squared_error: 0.7931 - val_loss: 0.4895 - val_root_mean_squared_error: 0.6996\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.6264 - root_mean_squared_error: 0.7915 - val_loss: 0.4895 - val_root_mean_squared_error: 0.6997\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.6190 - root_mean_squared_error: 0.7867 - val_loss: 0.4898 - val_root_mean_squared_error: 0.6998\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.6144 - root_mean_squared_error: 0.7838 - val_loss: 0.4899 - val_root_mean_squared_error: 0.6999\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.6064 - root_mean_squared_error: 0.7787 - val_loss: 0.4901 - val_root_mean_squared_error: 0.7001\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.6027 - root_mean_squared_error: 0.7763 - val_loss: 0.4906 - val_root_mean_squared_error: 0.7004\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.5978 - root_mean_squared_error: 0.7732 - val_loss: 0.4911 - val_root_mean_squared_error: 0.7008\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.5965 - root_mean_squared_error: 0.7723 - val_loss: 0.4916 - val_root_mean_squared_error: 0.7011\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.5891 - root_mean_squared_error: 0.7675 - val_loss: 0.4919 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.5882 - root_mean_squared_error: 0.7669 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_64 (Dense)            (None, 20)                40        \n",
      "                                                                 \n",
      " batch_normalization_31 (Ba  (None, 20)                80        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_61 (Activation)  (None, 20)                0         \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 20)                0         \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " batch_normalization_32 (Ba  (None, 20)                80        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_62 (Activation)  (None, 20)                0         \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 20)                0         \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      " activation_63 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 641 (2.50 KB)\n",
      "Trainable params: 561 (2.19 KB)\n",
      "Non-trainable params: 80 (320.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 4s 62ms/step - loss: 0.6672 - root_mean_squared_error: 0.8168 - val_loss: 0.5190 - val_root_mean_squared_error: 0.7204\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.6315 - root_mean_squared_error: 0.7947 - val_loss: 0.5118 - val_root_mean_squared_error: 0.7154\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.6098 - root_mean_squared_error: 0.7809 - val_loss: 0.5063 - val_root_mean_squared_error: 0.7116\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.5895 - root_mean_squared_error: 0.7678 - val_loss: 0.5022 - val_root_mean_squared_error: 0.7086\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.5756 - root_mean_squared_error: 0.7587 - val_loss: 0.4959 - val_root_mean_squared_error: 0.7042\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.5679 - root_mean_squared_error: 0.7536 - val_loss: 0.4931 - val_root_mean_squared_error: 0.7022\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.5593 - root_mean_squared_error: 0.7479 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.5553 - root_mean_squared_error: 0.7452 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.5459 - root_mean_squared_error: 0.7388 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.5380 - root_mean_squared_error: 0.7335 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.5329 - root_mean_squared_error: 0.7300 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.5329 - root_mean_squared_error: 0.7300 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.5289 - root_mean_squared_error: 0.7273 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.5268 - root_mean_squared_error: 0.7258 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.5247 - root_mean_squared_error: 0.7243 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.5208 - root_mean_squared_error: 0.7217 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.5204 - root_mean_squared_error: 0.7214 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.5175 - root_mean_squared_error: 0.7194 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.5175 - root_mean_squared_error: 0.7194 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.5163 - root_mean_squared_error: 0.7185 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.5156 - root_mean_squared_error: 0.7180 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.5158 - root_mean_squared_error: 0.7182 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.5144 - root_mean_squared_error: 0.7172 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.5112 - root_mean_squared_error: 0.7149 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.5126 - root_mean_squared_error: 0.7160 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.5107 - root_mean_squared_error: 0.7146 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.5124 - root_mean_squared_error: 0.7158 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.5102 - root_mean_squared_error: 0.7143 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.5103 - root_mean_squared_error: 0.7144 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.5101 - root_mean_squared_error: 0.7142 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_67 (Dense)            (None, 40)                80        \n",
      "                                                                 \n",
      " batch_normalization_33 (Ba  (None, 40)                160       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_64 (Activation)  (None, 40)                0         \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 40)                0         \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 40)                1640      \n",
      "                                                                 \n",
      " batch_normalization_34 (Ba  (None, 40)                160       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_65 (Activation)  (None, 40)                0         \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 40)                0         \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 1)                 41        \n",
      "                                                                 \n",
      " activation_66 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2081 (8.13 KB)\n",
      "Trainable params: 1921 (7.50 KB)\n",
      "Non-trainable params: 160 (640.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 4s 58ms/step - loss: 1.1593 - root_mean_squared_error: 1.0767 - val_loss: 0.5398 - val_root_mean_squared_error: 0.7347\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 1.0392 - root_mean_squared_error: 1.0194 - val_loss: 0.5089 - val_root_mean_squared_error: 0.7133\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9564 - root_mean_squared_error: 0.9779 - val_loss: 0.5019 - val_root_mean_squared_error: 0.7084\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.8929 - root_mean_squared_error: 0.9449 - val_loss: 0.4985 - val_root_mean_squared_error: 0.7061\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.8515 - root_mean_squared_error: 0.9228 - val_loss: 0.4956 - val_root_mean_squared_error: 0.7040\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.8052 - root_mean_squared_error: 0.8973 - val_loss: 0.4960 - val_root_mean_squared_error: 0.7043\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.7752 - root_mean_squared_error: 0.8805 - val_loss: 0.4949 - val_root_mean_squared_error: 0.7035\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.7332 - root_mean_squared_error: 0.8563 - val_loss: 0.4958 - val_root_mean_squared_error: 0.7041\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.7070 - root_mean_squared_error: 0.8408 - val_loss: 0.4979 - val_root_mean_squared_error: 0.7056\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.6902 - root_mean_squared_error: 0.8308 - val_loss: 0.5000 - val_root_mean_squared_error: 0.7071\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.6644 - root_mean_squared_error: 0.8151 - val_loss: 0.5004 - val_root_mean_squared_error: 0.7074\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.6509 - root_mean_squared_error: 0.8068 - val_loss: 0.4993 - val_root_mean_squared_error: 0.7066\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.6371 - root_mean_squared_error: 0.7982 - val_loss: 0.4980 - val_root_mean_squared_error: 0.7057\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.6223 - root_mean_squared_error: 0.7889 - val_loss: 0.4984 - val_root_mean_squared_error: 0.7060\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.6075 - root_mean_squared_error: 0.7794 - val_loss: 0.4993 - val_root_mean_squared_error: 0.7066\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.6007 - root_mean_squared_error: 0.7750 - val_loss: 0.4992 - val_root_mean_squared_error: 0.7065\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.5880 - root_mean_squared_error: 0.7668 - val_loss: 0.4987 - val_root_mean_squared_error: 0.7062\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.5794 - root_mean_squared_error: 0.7612 - val_loss: 0.4986 - val_root_mean_squared_error: 0.7061\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.5772 - root_mean_squared_error: 0.7598 - val_loss: 0.4985 - val_root_mean_squared_error: 0.7060\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.5718 - root_mean_squared_error: 0.7561 - val_loss: 0.4978 - val_root_mean_squared_error: 0.7056\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.5646 - root_mean_squared_error: 0.7514 - val_loss: 0.4969 - val_root_mean_squared_error: 0.7049\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.5604 - root_mean_squared_error: 0.7486 - val_loss: 0.4958 - val_root_mean_squared_error: 0.7041\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.5511 - root_mean_squared_error: 0.7423 - val_loss: 0.4948 - val_root_mean_squared_error: 0.7034\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.5521 - root_mean_squared_error: 0.7430 - val_loss: 0.4938 - val_root_mean_squared_error: 0.7027\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.5470 - root_mean_squared_error: 0.7396 - val_loss: 0.4927 - val_root_mean_squared_error: 0.7019\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.5423 - root_mean_squared_error: 0.7364 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.5384 - root_mean_squared_error: 0.7337 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.5381 - root_mean_squared_error: 0.7336 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.5338 - root_mean_squared_error: 0.7306 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.5358 - root_mean_squared_error: 0.7320 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_70 (Dense)            (None, 60)                120       \n",
      "                                                                 \n",
      " batch_normalization_35 (Ba  (None, 60)                240       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_67 (Activation)  (None, 60)                0         \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 60)                0         \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 60)                3660      \n",
      "                                                                 \n",
      " batch_normalization_36 (Ba  (None, 60)                240       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_68 (Activation)  (None, 60)                0         \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 60)                0         \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 1)                 61        \n",
      "                                                                 \n",
      " activation_69 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4321 (16.88 KB)\n",
      "Trainable params: 4081 (15.94 KB)\n",
      "Non-trainable params: 240 (960.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 5s 72ms/step - loss: 0.7556 - root_mean_squared_error: 0.8693 - val_loss: 0.4989 - val_root_mean_squared_error: 0.7063\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.6673 - root_mean_squared_error: 0.8169 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.6239 - root_mean_squared_error: 0.7899 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 1s 42ms/step - loss: 0.5942 - root_mean_squared_error: 0.7708 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.5795 - root_mean_squared_error: 0.7613 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.5670 - root_mean_squared_error: 0.7530 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.5573 - root_mean_squared_error: 0.7465 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.5448 - root_mean_squared_error: 0.7381 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 1s 44ms/step - loss: 0.5399 - root_mean_squared_error: 0.7348 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.5359 - root_mean_squared_error: 0.7320 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.5309 - root_mean_squared_error: 0.7286 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 0.5262 - root_mean_squared_error: 0.7254 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 0.5231 - root_mean_squared_error: 0.7233 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.5200 - root_mean_squared_error: 0.7211 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.5182 - root_mean_squared_error: 0.7199 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 1s 42ms/step - loss: 0.5168 - root_mean_squared_error: 0.7189 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.5157 - root_mean_squared_error: 0.7181 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.5134 - root_mean_squared_error: 0.7165 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.5123 - root_mean_squared_error: 0.7157 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 1s 44ms/step - loss: 0.5124 - root_mean_squared_error: 0.7158 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 0.5106 - root_mean_squared_error: 0.7146 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.5100 - root_mean_squared_error: 0.7141 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.5092 - root_mean_squared_error: 0.7136 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.5095 - root_mean_squared_error: 0.7138 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.5079 - root_mean_squared_error: 0.7127 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 1s 44ms/step - loss: 0.5080 - root_mean_squared_error: 0.7127 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.5057 - root_mean_squared_error: 0.7111 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.5076 - root_mean_squared_error: 0.7125 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 1s 43ms/step - loss: 0.5069 - root_mean_squared_error: 0.7119 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.5063 - root_mean_squared_error: 0.7116 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_73 (Dense)            (None, 80)                160       \n",
      "                                                                 \n",
      " batch_normalization_37 (Ba  (None, 80)                320       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_70 (Activation)  (None, 80)                0         \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 80)                0         \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_38 (Ba  (None, 80)                320       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_71 (Activation)  (None, 80)                0         \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 80)                0         \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 1)                 81        \n",
      "                                                                 \n",
      " activation_72 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7041 (27.50 KB)\n",
      "Non-trainable params: 320 (1.25 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 4s 71ms/step - loss: 1.2717 - root_mean_squared_error: 1.1277 - val_loss: 0.4909 - val_root_mean_squared_error: 0.7006\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 1.1141 - root_mean_squared_error: 1.0555 - val_loss: 0.4907 - val_root_mean_squared_error: 0.7005\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.9849 - root_mean_squared_error: 0.9924 - val_loss: 0.4937 - val_root_mean_squared_error: 0.7026\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.8911 - root_mean_squared_error: 0.9440 - val_loss: 0.4952 - val_root_mean_squared_error: 0.7037\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.8190 - root_mean_squared_error: 0.9050 - val_loss: 0.4949 - val_root_mean_squared_error: 0.7035\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.7594 - root_mean_squared_error: 0.8714 - val_loss: 0.4938 - val_root_mean_squared_error: 0.7027\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.7158 - root_mean_squared_error: 0.8461 - val_loss: 0.4926 - val_root_mean_squared_error: 0.7019\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.6714 - root_mean_squared_error: 0.8194 - val_loss: 0.4917 - val_root_mean_squared_error: 0.7012\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.6524 - root_mean_squared_error: 0.8077 - val_loss: 0.4918 - val_root_mean_squared_error: 0.7013\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.6276 - root_mean_squared_error: 0.7922 - val_loss: 0.4913 - val_root_mean_squared_error: 0.7009\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.6114 - root_mean_squared_error: 0.7819 - val_loss: 0.4916 - val_root_mean_squared_error: 0.7011\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.5965 - root_mean_squared_error: 0.7724 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.5846 - root_mean_squared_error: 0.7646 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.5732 - root_mean_squared_error: 0.7571 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.5642 - root_mean_squared_error: 0.7512 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.5585 - root_mean_squared_error: 0.7473 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.5514 - root_mean_squared_error: 0.7426 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.5430 - root_mean_squared_error: 0.7369 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.5417 - root_mean_squared_error: 0.7360 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.5359 - root_mean_squared_error: 0.7321 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.5361 - root_mean_squared_error: 0.7322 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.5329 - root_mean_squared_error: 0.7300 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.5287 - root_mean_squared_error: 0.7271 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.5253 - root_mean_squared_error: 0.7248 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.5251 - root_mean_squared_error: 0.7246 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.5222 - root_mean_squared_error: 0.7226 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.5207 - root_mean_squared_error: 0.7216 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.5200 - root_mean_squared_error: 0.7211 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.5178 - root_mean_squared_error: 0.7196 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.5170 - root_mean_squared_error: 0.7190 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_76 (Dense)            (None, 100)               200       \n",
      "                                                                 \n",
      " batch_normalization_39 (Ba  (None, 100)               400       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_73 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_40 (Ba  (None, 100)               400       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_74 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      " activation_75 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11201 (43.75 KB)\n",
      "Trainable params: 10801 (42.19 KB)\n",
      "Non-trainable params: 400 (1.56 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 5s 92ms/step - loss: 0.9059 - root_mean_squared_error: 0.9518 - val_loss: 0.5202 - val_root_mean_squared_error: 0.7213\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.7651 - root_mean_squared_error: 0.8747 - val_loss: 0.4919 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.6829 - root_mean_squared_error: 0.8264 - val_loss: 0.4909 - val_root_mean_squared_error: 0.7006\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.6370 - root_mean_squared_error: 0.7981 - val_loss: 0.4923 - val_root_mean_squared_error: 0.7016\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.6034 - root_mean_squared_error: 0.7768 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.5810 - root_mean_squared_error: 0.7622 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.5630 - root_mean_squared_error: 0.7503 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.5492 - root_mean_squared_error: 0.7411 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.5453 - root_mean_squared_error: 0.7384 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.5361 - root_mean_squared_error: 0.7322 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.5308 - root_mean_squared_error: 0.7285 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.5273 - root_mean_squared_error: 0.7262 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.5239 - root_mean_squared_error: 0.7238 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.5202 - root_mean_squared_error: 0.7212 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.5173 - root_mean_squared_error: 0.7192 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.5164 - root_mean_squared_error: 0.7186 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.5151 - root_mean_squared_error: 0.7177 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.5140 - root_mean_squared_error: 0.7169 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.5113 - root_mean_squared_error: 0.7151 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.5089 - root_mean_squared_error: 0.7134 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.5079 - root_mean_squared_error: 0.7127 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.5086 - root_mean_squared_error: 0.7132 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.5074 - root_mean_squared_error: 0.7123 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.5074 - root_mean_squared_error: 0.7123 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.5083 - root_mean_squared_error: 0.7130 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.5062 - root_mean_squared_error: 0.7115 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.5062 - root_mean_squared_error: 0.7115 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.5052 - root_mean_squared_error: 0.7108 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.5058 - root_mean_squared_error: 0.7112 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.5059 - root_mean_squared_error: 0.7113 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_79 (Dense)            (None, 120)               240       \n",
      "                                                                 \n",
      " batch_normalization_41 (Ba  (None, 120)               480       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_76 (Activation)  (None, 120)               0         \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_80 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_42 (Ba  (None, 120)               480       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_77 (Activation)  (None, 120)               0         \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 1)                 121       \n",
      "                                                                 \n",
      " activation_78 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15841 (61.88 KB)\n",
      "Trainable params: 15361 (60.00 KB)\n",
      "Non-trainable params: 480 (1.88 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 5s 93ms/step - loss: 0.6754 - root_mean_squared_error: 0.8218 - val_loss: 0.4919 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.5995 - root_mean_squared_error: 0.7743 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.5640 - root_mean_squared_error: 0.7510 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.5477 - root_mean_squared_error: 0.7401 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.5365 - root_mean_squared_error: 0.7324 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.5266 - root_mean_squared_error: 0.7257 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.5212 - root_mean_squared_error: 0.7220 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.5174 - root_mean_squared_error: 0.7193 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.5141 - root_mean_squared_error: 0.7170 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.5111 - root_mean_squared_error: 0.7149 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.5102 - root_mean_squared_error: 0.7143 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.5082 - root_mean_squared_error: 0.7129 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.5072 - root_mean_squared_error: 0.7122 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 0.5067 - root_mean_squared_error: 0.7119 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.5061 - root_mean_squared_error: 0.7114 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.5060 - root_mean_squared_error: 0.7113 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.5050 - root_mean_squared_error: 0.7106 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.5042 - root_mean_squared_error: 0.7100 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.5039 - root_mean_squared_error: 0.7099 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.5037 - root_mean_squared_error: 0.7097 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.5026 - root_mean_squared_error: 0.7090 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.5029 - root_mean_squared_error: 0.7092 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.5027 - root_mean_squared_error: 0.7090 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.5021 - root_mean_squared_error: 0.7086 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.5022 - root_mean_squared_error: 0.7086 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.5020 - root_mean_squared_error: 0.7085 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 0.5025 - root_mean_squared_error: 0.7089 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.5025 - root_mean_squared_error: 0.7089 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.5020 - root_mean_squared_error: 0.7085 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.5013 - root_mean_squared_error: 0.7080 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_82 (Dense)            (None, 140)               280       \n",
      "                                                                 \n",
      " batch_normalization_43 (Ba  (None, 140)               560       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_79 (Activation)  (None, 140)               0         \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 140)               0         \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 140)               19740     \n",
      "                                                                 \n",
      " batch_normalization_44 (Ba  (None, 140)               560       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_80 (Activation)  (None, 140)               0         \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, 140)               0         \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 1)                 141       \n",
      "                                                                 \n",
      " activation_81 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21281 (83.13 KB)\n",
      "Trainable params: 20721 (80.94 KB)\n",
      "Non-trainable params: 560 (2.19 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 5s 97ms/step - loss: 0.7700 - root_mean_squared_error: 0.8775 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.6665 - root_mean_squared_error: 0.8164 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.6128 - root_mean_squared_error: 0.7828 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.5746 - root_mean_squared_error: 0.7580 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.5510 - root_mean_squared_error: 0.7423 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.5379 - root_mean_squared_error: 0.7334 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.5316 - root_mean_squared_error: 0.7291 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.5269 - root_mean_squared_error: 0.7259 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.5207 - root_mean_squared_error: 0.7216 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.5161 - root_mean_squared_error: 0.7184 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.5124 - root_mean_squared_error: 0.7158 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.5113 - root_mean_squared_error: 0.7150 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.5102 - root_mean_squared_error: 0.7143 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.5088 - root_mean_squared_error: 0.7133 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.5072 - root_mean_squared_error: 0.7122 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.5073 - root_mean_squared_error: 0.7123 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.5053 - root_mean_squared_error: 0.7109 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.5050 - root_mean_squared_error: 0.7107 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.5051 - root_mean_squared_error: 0.7107 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 0.5039 - root_mean_squared_error: 0.7098 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.5036 - root_mean_squared_error: 0.7096 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.5039 - root_mean_squared_error: 0.7099 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.5031 - root_mean_squared_error: 0.7093 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.5034 - root_mean_squared_error: 0.7095 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 0.5023 - root_mean_squared_error: 0.7087 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.5020 - root_mean_squared_error: 0.7085 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.5018 - root_mean_squared_error: 0.7083 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.5026 - root_mean_squared_error: 0.7089 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.5010 - root_mean_squared_error: 0.7078 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 0.5014 - root_mean_squared_error: 0.7081 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_85 (Dense)            (None, 160)               320       \n",
      "                                                                 \n",
      " batch_normalization_45 (Ba  (None, 160)               640       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_82 (Activation)  (None, 160)               0         \n",
      "                                                                 \n",
      " dropout_44 (Dropout)        (None, 160)               0         \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 160)               25760     \n",
      "                                                                 \n",
      " batch_normalization_46 (Ba  (None, 160)               640       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_83 (Activation)  (None, 160)               0         \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 160)               0         \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 1)                 161       \n",
      "                                                                 \n",
      " activation_84 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27521 (107.50 KB)\n",
      "Trainable params: 26881 (105.00 KB)\n",
      "Non-trainable params: 640 (2.50 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 6s 142ms/step - loss: 0.5686 - root_mean_squared_error: 0.7540 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 0.5278 - root_mean_squared_error: 0.7265 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 0.5165 - root_mean_squared_error: 0.7187 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.5113 - root_mean_squared_error: 0.7150 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 0.5091 - root_mean_squared_error: 0.7135 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.5057 - root_mean_squared_error: 0.7111 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 0.5055 - root_mean_squared_error: 0.7110 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 0.5046 - root_mean_squared_error: 0.7103 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 0.5047 - root_mean_squared_error: 0.7104 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.5036 - root_mean_squared_error: 0.7096 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.5027 - root_mean_squared_error: 0.7090 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.5025 - root_mean_squared_error: 0.7089 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 0.5020 - root_mean_squared_error: 0.7085 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 0.5016 - root_mean_squared_error: 0.7082 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.5018 - root_mean_squared_error: 0.7084 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.5018 - root_mean_squared_error: 0.7084 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.5015 - root_mean_squared_error: 0.7082 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.5012 - root_mean_squared_error: 0.7080 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.5018 - root_mean_squared_error: 0.7084 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.5015 - root_mean_squared_error: 0.7081 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 0.5014 - root_mean_squared_error: 0.7081 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.5012 - root_mean_squared_error: 0.7079 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 0.5011 - root_mean_squared_error: 0.7079 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 0.5007 - root_mean_squared_error: 0.7076 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 0.5012 - root_mean_squared_error: 0.7080 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.5005 - root_mean_squared_error: 0.7075 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.5010 - root_mean_squared_error: 0.7078 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.5008 - root_mean_squared_error: 0.7077 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 0.5005 - root_mean_squared_error: 0.7074 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_88 (Dense)            (None, 180)               360       \n",
      "                                                                 \n",
      " batch_normalization_47 (Ba  (None, 180)               720       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_85 (Activation)  (None, 180)               0         \n",
      "                                                                 \n",
      " dropout_46 (Dropout)        (None, 180)               0         \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 180)               32580     \n",
      "                                                                 \n",
      " batch_normalization_48 (Ba  (None, 180)               720       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_86 (Activation)  (None, 180)               0         \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, 180)               0         \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 1)                 181       \n",
      "                                                                 \n",
      " activation_87 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34561 (135.00 KB)\n",
      "Trainable params: 33841 (132.19 KB)\n",
      "Non-trainable params: 720 (2.81 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 6s 127ms/step - loss: 0.8207 - root_mean_squared_error: 0.9059 - val_loss: 0.4918 - val_root_mean_squared_error: 0.7013\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 0.6856 - root_mean_squared_error: 0.8280 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 0.6159 - root_mean_squared_error: 0.7848 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 0.5725 - root_mean_squared_error: 0.7566 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 2s 92ms/step - loss: 0.5507 - root_mean_squared_error: 0.7421 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.5358 - root_mean_squared_error: 0.7320 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.5269 - root_mean_squared_error: 0.7259 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.5217 - root_mean_squared_error: 0.7223 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 0.5162 - root_mean_squared_error: 0.7185 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 0.5150 - root_mean_squared_error: 0.7176 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 0.5097 - root_mean_squared_error: 0.7139 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.5082 - root_mean_squared_error: 0.7129 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 0.5075 - root_mean_squared_error: 0.7124 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 0.5073 - root_mean_squared_error: 0.7122 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 0.5045 - root_mean_squared_error: 0.7103 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.5042 - root_mean_squared_error: 0.7100 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 0.5040 - root_mean_squared_error: 0.7099 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 0.5017 - root_mean_squared_error: 0.7083 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 0.5025 - root_mean_squared_error: 0.7089 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 0.5013 - root_mean_squared_error: 0.7081 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 0.5017 - root_mean_squared_error: 0.7083 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 0.5003 - root_mean_squared_error: 0.7073 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 0.4999 - root_mean_squared_error: 0.7071 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.5000 - root_mean_squared_error: 0.7071 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4995 - root_mean_squared_error: 0.7067 - val_loss: 0.4918 - val_root_mean_squared_error: 0.7013\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 0.4998 - root_mean_squared_error: 0.7069 - val_loss: 0.4915 - val_root_mean_squared_error: 0.7010\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 0.5000 - root_mean_squared_error: 0.7071 - val_loss: 0.4911 - val_root_mean_squared_error: 0.7008\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 0.4991 - root_mean_squared_error: 0.7064 - val_loss: 0.4907 - val_root_mean_squared_error: 0.7005\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.4984 - root_mean_squared_error: 0.7060 - val_loss: 0.4902 - val_root_mean_squared_error: 0.7002\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.4973 - root_mean_squared_error: 0.7052 - val_loss: 0.4896 - val_root_mean_squared_error: 0.6997\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.4983 - root_mean_squared_error: 0.7059\n",
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_91 (Dense)            (None, 200)               400       \n",
      "                                                                 \n",
      " batch_normalization_49 (Ba  (None, 200)               800       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_88 (Activation)  (None, 200)               0         \n",
      "                                                                 \n",
      " dropout_48 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 200)               40200     \n",
      "                                                                 \n",
      " batch_normalization_50 (Ba  (None, 200)               800       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_89 (Activation)  (None, 200)               0         \n",
      "                                                                 \n",
      " dropout_49 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 1)                 201       \n",
      "                                                                 \n",
      " activation_90 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42401 (165.63 KB)\n",
      "Trainable params: 41601 (162.50 KB)\n",
      "Non-trainable params: 800 (3.12 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2 hidden layers\n",
    "\n",
    "rmse_2 = []\n",
    "\n",
    "for i, value in enumerate(units_list):\n",
    "    \n",
    "    # train the nn model on training set\n",
    "    model = creat_model_2_layer(units_list[i]) \n",
    "    model.fit(X_train, y_train, batch_size = 3000, epochs=30, validation_split=0.2)\n",
    "    \n",
    "    # get prediction from testing set\n",
    "    score = model.evaluate(X_test, y_test)\n",
    "    rmse_2.append(score[1]) # score[0] is the loss\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "11851c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "16/16 [==============================] - 5s 50ms/step - loss: 1.2943 - root_mean_squared_error: 1.1377 - val_loss: 0.6164 - val_root_mean_squared_error: 0.7851\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 1.2204 - root_mean_squared_error: 1.1047 - val_loss: 0.5746 - val_root_mean_squared_error: 0.7580\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 1.1530 - root_mean_squared_error: 1.0738 - val_loss: 0.5521 - val_root_mean_squared_error: 0.7430\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 1.0891 - root_mean_squared_error: 1.0436 - val_loss: 0.5353 - val_root_mean_squared_error: 0.7317\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 1.0349 - root_mean_squared_error: 1.0173 - val_loss: 0.5242 - val_root_mean_squared_error: 0.7240\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.9919 - root_mean_squared_error: 0.9959 - val_loss: 0.5170 - val_root_mean_squared_error: 0.7191\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.9460 - root_mean_squared_error: 0.9726 - val_loss: 0.5135 - val_root_mean_squared_error: 0.7166\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.9115 - root_mean_squared_error: 0.9547 - val_loss: 0.5126 - val_root_mean_squared_error: 0.7160\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.8835 - root_mean_squared_error: 0.9399 - val_loss: 0.5117 - val_root_mean_squared_error: 0.7154\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.8523 - root_mean_squared_error: 0.9232 - val_loss: 0.5110 - val_root_mean_squared_error: 0.7148\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.8352 - root_mean_squared_error: 0.9139 - val_loss: 0.5105 - val_root_mean_squared_error: 0.7145\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.8140 - root_mean_squared_error: 0.9022 - val_loss: 0.5073 - val_root_mean_squared_error: 0.7122\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.7854 - root_mean_squared_error: 0.8862 - val_loss: 0.5036 - val_root_mean_squared_error: 0.7096\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.7664 - root_mean_squared_error: 0.8754 - val_loss: 0.4991 - val_root_mean_squared_error: 0.7064\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.7522 - root_mean_squared_error: 0.8673 - val_loss: 0.4948 - val_root_mean_squared_error: 0.7034\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.7416 - root_mean_squared_error: 0.8612 - val_loss: 0.4924 - val_root_mean_squared_error: 0.7017\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.7183 - root_mean_squared_error: 0.8475 - val_loss: 0.4910 - val_root_mean_squared_error: 0.7007\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.7056 - root_mean_squared_error: 0.8400 - val_loss: 0.4901 - val_root_mean_squared_error: 0.7001\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.6965 - root_mean_squared_error: 0.8346 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.6913 - root_mean_squared_error: 0.8314 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.6785 - root_mean_squared_error: 0.8237 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.6642 - root_mean_squared_error: 0.8150 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.6599 - root_mean_squared_error: 0.8124 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.6575 - root_mean_squared_error: 0.8108 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.6430 - root_mean_squared_error: 0.8019 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.6406 - root_mean_squared_error: 0.8004 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.6366 - root_mean_squared_error: 0.7978 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.6278 - root_mean_squared_error: 0.7923 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.6172 - root_mean_squared_error: 0.7856 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.6163 - root_mean_squared_error: 0.7851 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_94 (Dense)            (None, 20)                40        \n",
      "                                                                 \n",
      " batch_normalization_51 (Ba  (None, 20)                80        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_91 (Activation)  (None, 20)                0         \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 20)                0         \n",
      "                                                                 \n",
      " dense_95 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " batch_normalization_52 (Ba  (None, 20)                80        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_92 (Activation)  (None, 20)                0         \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 20)                0         \n",
      "                                                                 \n",
      " dense_96 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " batch_normalization_53 (Ba  (None, 20)                80        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_93 (Activation)  (None, 20)                0         \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 20)                0         \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      " activation_94 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1141 (4.46 KB)\n",
      "Trainable params: 1021 (3.99 KB)\n",
      "Non-trainable params: 120 (480.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 6s 69ms/step - loss: 1.9735 - root_mean_squared_error: 1.4048 - val_loss: 0.5915 - val_root_mean_squared_error: 0.7691\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 1.8027 - root_mean_squared_error: 1.3426 - val_loss: 0.5711 - val_root_mean_squared_error: 0.7557\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.6857 - root_mean_squared_error: 1.2984 - val_loss: 0.5614 - val_root_mean_squared_error: 0.7492\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 1.5948 - root_mean_squared_error: 1.2629 - val_loss: 0.5606 - val_root_mean_squared_error: 0.7487\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 1.4976 - root_mean_squared_error: 1.2237 - val_loss: 0.5687 - val_root_mean_squared_error: 0.7541\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 1.4149 - root_mean_squared_error: 1.1895 - val_loss: 0.5782 - val_root_mean_squared_error: 0.7604\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 1.3399 - root_mean_squared_error: 1.1575 - val_loss: 0.5827 - val_root_mean_squared_error: 0.7633\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 1.2777 - root_mean_squared_error: 1.1303 - val_loss: 0.5844 - val_root_mean_squared_error: 0.7644\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 1.2288 - root_mean_squared_error: 1.1085 - val_loss: 0.5868 - val_root_mean_squared_error: 0.7660\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 1.1571 - root_mean_squared_error: 1.0757 - val_loss: 0.5898 - val_root_mean_squared_error: 0.7680\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 1.1102 - root_mean_squared_error: 1.0537 - val_loss: 0.5911 - val_root_mean_squared_error: 0.7688\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 1.0579 - root_mean_squared_error: 1.0285 - val_loss: 0.5903 - val_root_mean_squared_error: 0.7683\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 1.0186 - root_mean_squared_error: 1.0093 - val_loss: 0.5895 - val_root_mean_squared_error: 0.7678\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.9804 - root_mean_squared_error: 0.9901 - val_loss: 0.5828 - val_root_mean_squared_error: 0.7634\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.9507 - root_mean_squared_error: 0.9751 - val_loss: 0.5752 - val_root_mean_squared_error: 0.7584\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.9140 - root_mean_squared_error: 0.9560 - val_loss: 0.5694 - val_root_mean_squared_error: 0.7546\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.8880 - root_mean_squared_error: 0.9424 - val_loss: 0.5642 - val_root_mean_squared_error: 0.7511\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 1s 31ms/step - loss: 0.8580 - root_mean_squared_error: 0.9263 - val_loss: 0.5570 - val_root_mean_squared_error: 0.7463\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.8398 - root_mean_squared_error: 0.9164 - val_loss: 0.5482 - val_root_mean_squared_error: 0.7404\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.8155 - root_mean_squared_error: 0.9030 - val_loss: 0.5421 - val_root_mean_squared_error: 0.7363\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.7924 - root_mean_squared_error: 0.8902 - val_loss: 0.5363 - val_root_mean_squared_error: 0.7323\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.7671 - root_mean_squared_error: 0.8759 - val_loss: 0.5305 - val_root_mean_squared_error: 0.7283\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.7575 - root_mean_squared_error: 0.8703 - val_loss: 0.5221 - val_root_mean_squared_error: 0.7225\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 0.7404 - root_mean_squared_error: 0.8605 - val_loss: 0.5147 - val_root_mean_squared_error: 0.7174\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.7250 - root_mean_squared_error: 0.8515 - val_loss: 0.5095 - val_root_mean_squared_error: 0.7138\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.7155 - root_mean_squared_error: 0.8459 - val_loss: 0.5064 - val_root_mean_squared_error: 0.7116\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.6944 - root_mean_squared_error: 0.8333 - val_loss: 0.5030 - val_root_mean_squared_error: 0.7092\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.6897 - root_mean_squared_error: 0.8305 - val_loss: 0.5004 - val_root_mean_squared_error: 0.7074\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.6747 - root_mean_squared_error: 0.8214 - val_loss: 0.4984 - val_root_mean_squared_error: 0.7059\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.6651 - root_mean_squared_error: 0.8155 - val_loss: 0.4986 - val_root_mean_squared_error: 0.7061\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.5092 - root_mean_squared_error: 0.7136\n",
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_98 (Dense)            (None, 40)                80        \n",
      "                                                                 \n",
      " batch_normalization_54 (Ba  (None, 40)                160       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_95 (Activation)  (None, 40)                0         \n",
      "                                                                 \n",
      " dropout_53 (Dropout)        (None, 40)                0         \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 40)                1640      \n",
      "                                                                 \n",
      " batch_normalization_55 (Ba  (None, 40)                160       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_96 (Activation)  (None, 40)                0         \n",
      "                                                                 \n",
      " dropout_54 (Dropout)        (None, 40)                0         \n",
      "                                                                 \n",
      " dense_100 (Dense)           (None, 40)                1640      \n",
      "                                                                 \n",
      " batch_normalization_56 (Ba  (None, 40)                160       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_97 (Activation)  (None, 40)                0         \n",
      "                                                                 \n",
      " dropout_55 (Dropout)        (None, 40)                0         \n",
      "                                                                 \n",
      " dense_101 (Dense)           (None, 1)                 41        \n",
      "                                                                 \n",
      " activation_98 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3881 (15.16 KB)\n",
      "Trainable params: 3641 (14.22 KB)\n",
      "Non-trainable params: 240 (960.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 5s 72ms/step - loss: 0.8023 - root_mean_squared_error: 0.8957 - val_loss: 0.4942 - val_root_mean_squared_error: 0.7030\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.7495 - root_mean_squared_error: 0.8658 - val_loss: 0.4919 - val_root_mean_squared_error: 0.7013\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.7118 - root_mean_squared_error: 0.8437 - val_loss: 0.4917 - val_root_mean_squared_error: 0.7012\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 1s 42ms/step - loss: 0.6776 - root_mean_squared_error: 0.8232 - val_loss: 0.4911 - val_root_mean_squared_error: 0.7008\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.6426 - root_mean_squared_error: 0.8016 - val_loss: 0.4931 - val_root_mean_squared_error: 0.7022\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.6245 - root_mean_squared_error: 0.7903 - val_loss: 0.4956 - val_root_mean_squared_error: 0.7040\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.6055 - root_mean_squared_error: 0.7781 - val_loss: 0.4952 - val_root_mean_squared_error: 0.7037\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 1s 44ms/step - loss: 0.5957 - root_mean_squared_error: 0.7718 - val_loss: 0.4930 - val_root_mean_squared_error: 0.7022\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.5801 - root_mean_squared_error: 0.7617 - val_loss: 0.4919 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 1s 44ms/step - loss: 0.5727 - root_mean_squared_error: 0.7568 - val_loss: 0.4917 - val_root_mean_squared_error: 0.7012\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.5646 - root_mean_squared_error: 0.7514 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.5571 - root_mean_squared_error: 0.7464 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.5474 - root_mean_squared_error: 0.7398 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.5444 - root_mean_squared_error: 0.7379 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.5405 - root_mean_squared_error: 0.7352 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.5375 - root_mean_squared_error: 0.7332 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.5351 - root_mean_squared_error: 0.7315 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.5316 - root_mean_squared_error: 0.7291 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.5280 - root_mean_squared_error: 0.7266 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.5257 - root_mean_squared_error: 0.7251 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.5233 - root_mean_squared_error: 0.7234 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.5213 - root_mean_squared_error: 0.7220 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.5196 - root_mean_squared_error: 0.7209 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.5196 - root_mean_squared_error: 0.7208 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.5172 - root_mean_squared_error: 0.7191 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.5164 - root_mean_squared_error: 0.7186 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.5146 - root_mean_squared_error: 0.7173 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.5146 - root_mean_squared_error: 0.7174 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.5135 - root_mean_squared_error: 0.7166 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.5131 - root_mean_squared_error: 0.7163 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_102 (Dense)           (None, 60)                120       \n",
      "                                                                 \n",
      " batch_normalization_57 (Ba  (None, 60)                240       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_99 (Activation)  (None, 60)                0         \n",
      "                                                                 \n",
      " dropout_56 (Dropout)        (None, 60)                0         \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 60)                3660      \n",
      "                                                                 \n",
      " batch_normalization_58 (Ba  (None, 60)                240       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_100 (Activation  (None, 60)                0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_57 (Dropout)        (None, 60)                0         \n",
      "                                                                 \n",
      " dense_104 (Dense)           (None, 60)                3660      \n",
      "                                                                 \n",
      " batch_normalization_59 (Ba  (None, 60)                240       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_101 (Activation  (None, 60)                0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_58 (Dropout)        (None, 60)                0         \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 1)                 61        \n",
      "                                                                 \n",
      " activation_102 (Activation  (None, 1)                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8221 (32.11 KB)\n",
      "Trainable params: 7861 (30.71 KB)\n",
      "Non-trainable params: 360 (1.41 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 6s 91ms/step - loss: 1.9944 - root_mean_squared_error: 1.4122 - val_loss: 0.5232 - val_root_mean_squared_error: 0.7233\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 1.7147 - root_mean_squared_error: 1.3095 - val_loss: 0.4976 - val_root_mean_squared_error: 0.7054\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 1.5296 - root_mean_squared_error: 1.2368 - val_loss: 0.4948 - val_root_mean_squared_error: 0.7035\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 1.3806 - root_mean_squared_error: 1.1750 - val_loss: 0.5025 - val_root_mean_squared_error: 0.7088\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 1.2487 - root_mean_squared_error: 1.1175 - val_loss: 0.5037 - val_root_mean_squared_error: 0.7097\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 1.1512 - root_mean_squared_error: 1.0730 - val_loss: 0.5041 - val_root_mean_squared_error: 0.7100\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 1.0602 - root_mean_squared_error: 1.0297 - val_loss: 0.5030 - val_root_mean_squared_error: 0.7092\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 0.9972 - root_mean_squared_error: 0.9986 - val_loss: 0.5023 - val_root_mean_squared_error: 0.7087\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.9280 - root_mean_squared_error: 0.9633 - val_loss: 0.5027 - val_root_mean_squared_error: 0.7090\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.8760 - root_mean_squared_error: 0.9360 - val_loss: 0.5031 - val_root_mean_squared_error: 0.7093\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 0.8472 - root_mean_squared_error: 0.9204 - val_loss: 0.5026 - val_root_mean_squared_error: 0.7089\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.8108 - root_mean_squared_error: 0.9005 - val_loss: 0.5023 - val_root_mean_squared_error: 0.7088\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 0.7748 - root_mean_squared_error: 0.8802 - val_loss: 0.5042 - val_root_mean_squared_error: 0.7101\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 0.7382 - root_mean_squared_error: 0.8592 - val_loss: 0.5052 - val_root_mean_squared_error: 0.7108\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 0.7188 - root_mean_squared_error: 0.8478 - val_loss: 0.5037 - val_root_mean_squared_error: 0.7097\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 0.6994 - root_mean_squared_error: 0.8363 - val_loss: 0.5028 - val_root_mean_squared_error: 0.7091\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 0.6744 - root_mean_squared_error: 0.8212 - val_loss: 0.5004 - val_root_mean_squared_error: 0.7074\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.6633 - root_mean_squared_error: 0.8145 - val_loss: 0.4978 - val_root_mean_squared_error: 0.7055\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.6454 - root_mean_squared_error: 0.8034 - val_loss: 0.4963 - val_root_mean_squared_error: 0.7045\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.6406 - root_mean_squared_error: 0.8004 - val_loss: 0.4934 - val_root_mean_squared_error: 0.7024\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.6212 - root_mean_squared_error: 0.7882 - val_loss: 0.4923 - val_root_mean_squared_error: 0.7017\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.6129 - root_mean_squared_error: 0.7829 - val_loss: 0.4910 - val_root_mean_squared_error: 0.7007\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.6071 - root_mean_squared_error: 0.7792 - val_loss: 0.4915 - val_root_mean_squared_error: 0.7011\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.6010 - root_mean_squared_error: 0.7753 - val_loss: 0.4927 - val_root_mean_squared_error: 0.7019\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 0.5898 - root_mean_squared_error: 0.7680 - val_loss: 0.4937 - val_root_mean_squared_error: 0.7026\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.5858 - root_mean_squared_error: 0.7654 - val_loss: 0.4930 - val_root_mean_squared_error: 0.7022\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.5779 - root_mean_squared_error: 0.7602 - val_loss: 0.4925 - val_root_mean_squared_error: 0.7018\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.5711 - root_mean_squared_error: 0.7557 - val_loss: 0.4922 - val_root_mean_squared_error: 0.7016\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.5712 - root_mean_squared_error: 0.7558 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.5650 - root_mean_squared_error: 0.7517 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_106 (Dense)           (None, 80)                160       \n",
      "                                                                 \n",
      " batch_normalization_60 (Ba  (None, 80)                320       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_103 (Activation  (None, 80)                0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_59 (Dropout)        (None, 80)                0         \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_61 (Ba  (None, 80)                320       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_104 (Activation  (None, 80)                0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_60 (Dropout)        (None, 80)                0         \n",
      "                                                                 \n",
      " dense_108 (Dense)           (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_62 (Ba  (None, 80)                320       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_105 (Activation  (None, 80)                0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_61 (Dropout)        (None, 80)                0         \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 1)                 81        \n",
      "                                                                 \n",
      " activation_106 (Activation  (None, 1)                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14161 (55.32 KB)\n",
      "Trainable params: 13681 (53.44 KB)\n",
      "Non-trainable params: 480 (1.88 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 6s 111ms/step - loss: 0.5493 - root_mean_squared_error: 0.7411 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 0.5301 - root_mean_squared_error: 0.7281 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 0.5192 - root_mean_squared_error: 0.7206 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.5129 - root_mean_squared_error: 0.7161 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.5100 - root_mean_squared_error: 0.7141 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.5081 - root_mean_squared_error: 0.7128 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 0.5070 - root_mean_squared_error: 0.7120 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 0.5055 - root_mean_squared_error: 0.7110 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.5039 - root_mean_squared_error: 0.7099 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 0.5049 - root_mean_squared_error: 0.7106 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.5035 - root_mean_squared_error: 0.7096 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 0.5034 - root_mean_squared_error: 0.7095 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.5032 - root_mean_squared_error: 0.7094 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 0.5026 - root_mean_squared_error: 0.7090 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.5024 - root_mean_squared_error: 0.7088 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.5024 - root_mean_squared_error: 0.7088 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.5020 - root_mean_squared_error: 0.7086 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.5026 - root_mean_squared_error: 0.7090 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.5020 - root_mean_squared_error: 0.7085 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.5016 - root_mean_squared_error: 0.7083 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.5023 - root_mean_squared_error: 0.7087 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.5015 - root_mean_squared_error: 0.7081 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.5015 - root_mean_squared_error: 0.7081 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.5013 - root_mean_squared_error: 0.7080 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.5016 - root_mean_squared_error: 0.7082 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 0.5014 - root_mean_squared_error: 0.7081 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 0.5012 - root_mean_squared_error: 0.7079 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 0.5018 - root_mean_squared_error: 0.7084 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 0.5010 - root_mean_squared_error: 0.7078 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.5010 - root_mean_squared_error: 0.7078 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.5009 - root_mean_squared_error: 0.7077\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_110 (Dense)           (None, 100)               200       \n",
      "                                                                 \n",
      " batch_normalization_63 (Ba  (None, 100)               400       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_107 (Activation  (None, 100)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_62 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_111 (Dense)           (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_64 (Ba  (None, 100)               400       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_108 (Activation  (None, 100)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_63 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_112 (Dense)           (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_65 (Ba  (None, 100)               400       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_109 (Activation  (None, 100)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_64 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 1)                 101       \n",
      "                                                                 \n",
      " activation_110 (Activation  (None, 1)                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21701 (84.77 KB)\n",
      "Trainable params: 21101 (82.43 KB)\n",
      "Non-trainable params: 600 (2.34 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 7s 128ms/step - loss: 1.4768 - root_mean_squared_error: 1.2152 - val_loss: 0.5027 - val_root_mean_squared_error: 0.7090\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 1.2480 - root_mean_squared_error: 1.1172 - val_loss: 0.4975 - val_root_mean_squared_error: 0.7054\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 1.0746 - root_mean_squared_error: 1.0366 - val_loss: 0.4999 - val_root_mean_squared_error: 0.7070\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 0.9589 - root_mean_squared_error: 0.9792 - val_loss: 0.4992 - val_root_mean_squared_error: 0.7065\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 0.8656 - root_mean_squared_error: 0.9304 - val_loss: 0.4977 - val_root_mean_squared_error: 0.7055\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 0.7906 - root_mean_squared_error: 0.8892 - val_loss: 0.4949 - val_root_mean_squared_error: 0.7035\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.7397 - root_mean_squared_error: 0.8601 - val_loss: 0.4916 - val_root_mean_squared_error: 0.7011\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 0.6991 - root_mean_squared_error: 0.8361 - val_loss: 0.4894 - val_root_mean_squared_error: 0.6996\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.6652 - root_mean_squared_error: 0.8156 - val_loss: 0.4914 - val_root_mean_squared_error: 0.7010\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 0.6364 - root_mean_squared_error: 0.7978 - val_loss: 0.4907 - val_root_mean_squared_error: 0.7005\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.6212 - root_mean_squared_error: 0.7882 - val_loss: 0.4917 - val_root_mean_squared_error: 0.7012\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 0.6040 - root_mean_squared_error: 0.7771 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.5830 - root_mean_squared_error: 0.7636 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 0.5736 - root_mean_squared_error: 0.7574 - val_loss: 0.4917 - val_root_mean_squared_error: 0.7012\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.5633 - root_mean_squared_error: 0.7505 - val_loss: 0.4917 - val_root_mean_squared_error: 0.7012\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 0.5570 - root_mean_squared_error: 0.7463 - val_loss: 0.4916 - val_root_mean_squared_error: 0.7011\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 0.5517 - root_mean_squared_error: 0.7428 - val_loss: 0.4909 - val_root_mean_squared_error: 0.7007\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 0.5483 - root_mean_squared_error: 0.7405 - val_loss: 0.4909 - val_root_mean_squared_error: 0.7006\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.5422 - root_mean_squared_error: 0.7363 - val_loss: 0.4912 - val_root_mean_squared_error: 0.7009\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.5358 - root_mean_squared_error: 0.7320 - val_loss: 0.4906 - val_root_mean_squared_error: 0.7004\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 0.5333 - root_mean_squared_error: 0.7302 - val_loss: 0.4902 - val_root_mean_squared_error: 0.7001\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 0.5307 - root_mean_squared_error: 0.7285 - val_loss: 0.4901 - val_root_mean_squared_error: 0.7001\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 0.5244 - root_mean_squared_error: 0.7241 - val_loss: 0.4899 - val_root_mean_squared_error: 0.6999\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 0.5247 - root_mean_squared_error: 0.7244 - val_loss: 0.4889 - val_root_mean_squared_error: 0.6992\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 0.5214 - root_mean_squared_error: 0.7221 - val_loss: 0.4882 - val_root_mean_squared_error: 0.6987\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 0.5221 - root_mean_squared_error: 0.7226 - val_loss: 0.4876 - val_root_mean_squared_error: 0.6983\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 0.5178 - root_mean_squared_error: 0.7196 - val_loss: 0.4875 - val_root_mean_squared_error: 0.6982\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 0.5178 - root_mean_squared_error: 0.7196 - val_loss: 0.4873 - val_root_mean_squared_error: 0.6981\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.5139 - root_mean_squared_error: 0.7169 - val_loss: 0.4863 - val_root_mean_squared_error: 0.6974\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 0.5135 - root_mean_squared_error: 0.7166 - val_loss: 0.4852 - val_root_mean_squared_error: 0.6965\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.4935 - root_mean_squared_error: 0.7025\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_114 (Dense)           (None, 120)               240       \n",
      "                                                                 \n",
      " batch_normalization_66 (Ba  (None, 120)               480       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_111 (Activation  (None, 120)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_65 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_115 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_67 (Ba  (None, 120)               480       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_112 (Activation  (None, 120)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_66 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_116 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_68 (Ba  (None, 120)               480       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_113 (Activation  (None, 120)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_67 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_117 (Dense)           (None, 1)                 121       \n",
      "                                                                 \n",
      " activation_114 (Activation  (None, 1)                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30841 (120.47 KB)\n",
      "Trainable params: 30121 (117.66 KB)\n",
      "Non-trainable params: 720 (2.81 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 7s 155ms/step - loss: 0.8076 - root_mean_squared_error: 0.8987 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.7024 - root_mean_squared_error: 0.8381 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.6336 - root_mean_squared_error: 0.7960 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.5937 - root_mean_squared_error: 0.7705 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.5684 - root_mean_squared_error: 0.7540 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 0.5533 - root_mean_squared_error: 0.7439 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.5395 - root_mean_squared_error: 0.7345 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.5309 - root_mean_squared_error: 0.7287 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 0.5289 - root_mean_squared_error: 0.7273 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.5241 - root_mean_squared_error: 0.7240 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5199 - root_mean_squared_error: 0.7210 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.5179 - root_mean_squared_error: 0.7196 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 0.5126 - root_mean_squared_error: 0.7160 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.5138 - root_mean_squared_error: 0.7168 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5105 - root_mean_squared_error: 0.7145 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 0.5079 - root_mean_squared_error: 0.7127 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.5085 - root_mean_squared_error: 0.7131 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5071 - root_mean_squared_error: 0.7121 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 0.5067 - root_mean_squared_error: 0.7118 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.5056 - root_mean_squared_error: 0.7110 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.5041 - root_mean_squared_error: 0.7100 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 0.5033 - root_mean_squared_error: 0.7094 - val_loss: 0.4919 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5028 - root_mean_squared_error: 0.7091 - val_loss: 0.4908 - val_root_mean_squared_error: 0.7005\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.5029 - root_mean_squared_error: 0.7091 - val_loss: 0.4896 - val_root_mean_squared_error: 0.6997\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5020 - root_mean_squared_error: 0.7085 - val_loss: 0.4868 - val_root_mean_squared_error: 0.6977\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 0.5010 - root_mean_squared_error: 0.7078 - val_loss: 0.4848 - val_root_mean_squared_error: 0.6963\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.4994 - root_mean_squared_error: 0.7067 - val_loss: 0.4811 - val_root_mean_squared_error: 0.6936\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.4995 - root_mean_squared_error: 0.7068 - val_loss: 0.4797 - val_root_mean_squared_error: 0.6926\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.4983 - root_mean_squared_error: 0.7059 - val_loss: 0.4769 - val_root_mean_squared_error: 0.6906\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.4984 - root_mean_squared_error: 0.7060 - val_loss: 0.4769 - val_root_mean_squared_error: 0.6906\n",
      "1875/1875 [==============================] - 9s 4ms/step - loss: 0.4847 - root_mean_squared_error: 0.6962\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_118 (Dense)           (None, 140)               280       \n",
      "                                                                 \n",
      " batch_normalization_69 (Ba  (None, 140)               560       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_115 (Activation  (None, 140)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_68 (Dropout)        (None, 140)               0         \n",
      "                                                                 \n",
      " dense_119 (Dense)           (None, 140)               19740     \n",
      "                                                                 \n",
      " batch_normalization_70 (Ba  (None, 140)               560       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_116 (Activation  (None, 140)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_69 (Dropout)        (None, 140)               0         \n",
      "                                                                 \n",
      " dense_120 (Dense)           (None, 140)               19740     \n",
      "                                                                 \n",
      " batch_normalization_71 (Ba  (None, 140)               560       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_117 (Activation  (None, 140)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_70 (Dropout)        (None, 140)               0         \n",
      "                                                                 \n",
      " dense_121 (Dense)           (None, 1)                 141       \n",
      "                                                                 \n",
      " activation_118 (Activation  (None, 1)                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41581 (162.43 KB)\n",
      "Trainable params: 40741 (159.14 KB)\n",
      "Non-trainable params: 840 (3.28 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 8s 157ms/step - loss: 0.6928 - root_mean_squared_error: 0.8324 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 0.6082 - root_mean_squared_error: 0.7799 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 0.5696 - root_mean_squared_error: 0.7547 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5425 - root_mean_squared_error: 0.7365 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.5302 - root_mean_squared_error: 0.7281 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5225 - root_mean_squared_error: 0.7229 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.5170 - root_mean_squared_error: 0.7190 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.5122 - root_mean_squared_error: 0.7157 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.5095 - root_mean_squared_error: 0.7138 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 2s 93ms/step - loss: 0.5085 - root_mean_squared_error: 0.7131 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.5073 - root_mean_squared_error: 0.7122 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.5046 - root_mean_squared_error: 0.7103 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 0.5016 - root_mean_squared_error: 0.7082 - val_loss: 0.4901 - val_root_mean_squared_error: 0.7000\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.5015 - root_mean_squared_error: 0.7081 - val_loss: 0.4873 - val_root_mean_squared_error: 0.6981\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.5001 - root_mean_squared_error: 0.7072 - val_loss: 0.4840 - val_root_mean_squared_error: 0.6957\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.4995 - root_mean_squared_error: 0.7067 - val_loss: 0.4808 - val_root_mean_squared_error: 0.6934\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 0.4981 - root_mean_squared_error: 0.7058 - val_loss: 0.4765 - val_root_mean_squared_error: 0.6903\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.4964 - root_mean_squared_error: 0.7046 - val_loss: 0.4746 - val_root_mean_squared_error: 0.6889\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.4960 - root_mean_squared_error: 0.7043 - val_loss: 0.4713 - val_root_mean_squared_error: 0.6865\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4963 - root_mean_squared_error: 0.7045 - val_loss: 0.4704 - val_root_mean_squared_error: 0.6859\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.4932 - root_mean_squared_error: 0.7023 - val_loss: 0.4690 - val_root_mean_squared_error: 0.6849\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.4933 - root_mean_squared_error: 0.7023 - val_loss: 0.4686 - val_root_mean_squared_error: 0.6846\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.4938 - root_mean_squared_error: 0.7027 - val_loss: 0.4683 - val_root_mean_squared_error: 0.6843\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.4916 - root_mean_squared_error: 0.7012 - val_loss: 0.4673 - val_root_mean_squared_error: 0.6836\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.4918 - root_mean_squared_error: 0.7013 - val_loss: 0.4670 - val_root_mean_squared_error: 0.6833\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.4913 - root_mean_squared_error: 0.7009 - val_loss: 0.4665 - val_root_mean_squared_error: 0.6830\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 0.4905 - root_mean_squared_error: 0.7004 - val_loss: 0.4666 - val_root_mean_squared_error: 0.6831\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 0.4894 - root_mean_squared_error: 0.6996 - val_loss: 0.4663 - val_root_mean_squared_error: 0.6828\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.4892 - root_mean_squared_error: 0.6994 - val_loss: 0.4662 - val_root_mean_squared_error: 0.6828\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 0.4886 - root_mean_squared_error: 0.6990 - val_loss: 0.4659 - val_root_mean_squared_error: 0.6826\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.4731 - root_mean_squared_error: 0.6878\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_122 (Dense)           (None, 160)               320       \n",
      "                                                                 \n",
      " batch_normalization_72 (Ba  (None, 160)               640       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_119 (Activation  (None, 160)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_71 (Dropout)        (None, 160)               0         \n",
      "                                                                 \n",
      " dense_123 (Dense)           (None, 160)               25760     \n",
      "                                                                 \n",
      " batch_normalization_73 (Ba  (None, 160)               640       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_120 (Activation  (None, 160)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_72 (Dropout)        (None, 160)               0         \n",
      "                                                                 \n",
      " dense_124 (Dense)           (None, 160)               25760     \n",
      "                                                                 \n",
      " batch_normalization_74 (Ba  (None, 160)               640       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_121 (Activation  (None, 160)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_73 (Dropout)        (None, 160)               0         \n",
      "                                                                 \n",
      " dense_125 (Dense)           (None, 1)                 161       \n",
      "                                                                 \n",
      " activation_122 (Activation  (None, 1)                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53921 (210.63 KB)\n",
      "Trainable params: 52961 (206.88 KB)\n",
      "Non-trainable params: 960 (3.75 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 8s 160ms/step - loss: 1.0349 - root_mean_squared_error: 1.0173 - val_loss: 0.4906 - val_root_mean_squared_error: 0.7004\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 0.8120 - root_mean_squared_error: 0.9011 - val_loss: 0.4919 - val_root_mean_squared_error: 0.7013\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.7060 - root_mean_squared_error: 0.8402 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.6398 - root_mean_squared_error: 0.7999 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 0.6053 - root_mean_squared_error: 0.7780 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.5807 - root_mean_squared_error: 0.7620 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 0.5642 - root_mean_squared_error: 0.7512 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.5478 - root_mean_squared_error: 0.7401 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.5415 - root_mean_squared_error: 0.7359 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 2s 122ms/step - loss: 0.5308 - root_mean_squared_error: 0.7286 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 0.5283 - root_mean_squared_error: 0.7269 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.5245 - root_mean_squared_error: 0.7242 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 0.5212 - root_mean_squared_error: 0.7219 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 2s 122ms/step - loss: 0.5177 - root_mean_squared_error: 0.7195 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.5149 - root_mean_squared_error: 0.7176 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.5127 - root_mean_squared_error: 0.7160 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.5101 - root_mean_squared_error: 0.7142 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 0.5095 - root_mean_squared_error: 0.7138 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.5081 - root_mean_squared_error: 0.7128 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.5072 - root_mean_squared_error: 0.7122 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.5059 - root_mean_squared_error: 0.7112 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.5048 - root_mean_squared_error: 0.7105 - val_loss: 0.4918 - val_root_mean_squared_error: 0.7013\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.5039 - root_mean_squared_error: 0.7099 - val_loss: 0.4892 - val_root_mean_squared_error: 0.6994\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.5029 - root_mean_squared_error: 0.7092 - val_loss: 0.4879 - val_root_mean_squared_error: 0.6985\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.5025 - root_mean_squared_error: 0.7089 - val_loss: 0.4871 - val_root_mean_squared_error: 0.6980\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.5014 - root_mean_squared_error: 0.7081 - val_loss: 0.4838 - val_root_mean_squared_error: 0.6956\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.5007 - root_mean_squared_error: 0.7076 - val_loss: 0.4833 - val_root_mean_squared_error: 0.6952\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.4998 - root_mean_squared_error: 0.7070 - val_loss: 0.4815 - val_root_mean_squared_error: 0.6939\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.4990 - root_mean_squared_error: 0.7064 - val_loss: 0.4775 - val_root_mean_squared_error: 0.6910\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.4979 - root_mean_squared_error: 0.7056 - val_loss: 0.4769 - val_root_mean_squared_error: 0.6906\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.4846 - root_mean_squared_error: 0.6962\n",
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_126 (Dense)           (None, 180)               360       \n",
      "                                                                 \n",
      " batch_normalization_75 (Ba  (None, 180)               720       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_123 (Activation  (None, 180)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_74 (Dropout)        (None, 180)               0         \n",
      "                                                                 \n",
      " dense_127 (Dense)           (None, 180)               32580     \n",
      "                                                                 \n",
      " batch_normalization_76 (Ba  (None, 180)               720       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_124 (Activation  (None, 180)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_75 (Dropout)        (None, 180)               0         \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 180)               32580     \n",
      "                                                                 \n",
      " batch_normalization_77 (Ba  (None, 180)               720       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_125 (Activation  (None, 180)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_76 (Dropout)        (None, 180)               0         \n",
      "                                                                 \n",
      " dense_129 (Dense)           (None, 1)                 181       \n",
      "                                                                 \n",
      " activation_126 (Activation  (None, 1)                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67861 (265.08 KB)\n",
      "Trainable params: 66781 (260.86 KB)\n",
      "Non-trainable params: 1080 (4.22 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 8s 169ms/step - loss: 1.1379 - root_mean_squared_error: 1.0667 - val_loss: 0.4945 - val_root_mean_squared_error: 0.7032\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 0.9039 - root_mean_squared_error: 0.9508 - val_loss: 0.4972 - val_root_mean_squared_error: 0.7051\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 0.7464 - root_mean_squared_error: 0.8640 - val_loss: 0.4972 - val_root_mean_squared_error: 0.7051\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.6624 - root_mean_squared_error: 0.8139 - val_loss: 0.4950 - val_root_mean_squared_error: 0.7036\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 0.6105 - root_mean_squared_error: 0.7813 - val_loss: 0.4927 - val_root_mean_squared_error: 0.7019\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.5850 - root_mean_squared_error: 0.7649 - val_loss: 0.4946 - val_root_mean_squared_error: 0.7033\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 0.5646 - root_mean_squared_error: 0.7514 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 2s 122ms/step - loss: 0.5500 - root_mean_squared_error: 0.7416 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.5359 - root_mean_squared_error: 0.7321 - val_loss: 0.4920 - val_root_mean_squared_error: 0.7014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 2s 122ms/step - loss: 0.5323 - root_mean_squared_error: 0.7296 - val_loss: 0.4912 - val_root_mean_squared_error: 0.7009\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 0.5261 - root_mean_squared_error: 0.7253 - val_loss: 0.4907 - val_root_mean_squared_error: 0.7005\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 0.5209 - root_mean_squared_error: 0.7218 - val_loss: 0.4883 - val_root_mean_squared_error: 0.6988\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.5147 - root_mean_squared_error: 0.7174 - val_loss: 0.4865 - val_root_mean_squared_error: 0.6975\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.5120 - root_mean_squared_error: 0.7155 - val_loss: 0.4835 - val_root_mean_squared_error: 0.6953\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 0.5098 - root_mean_squared_error: 0.7140 - val_loss: 0.4807 - val_root_mean_squared_error: 0.6933\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.5071 - root_mean_squared_error: 0.7121 - val_loss: 0.4785 - val_root_mean_squared_error: 0.6917\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.5048 - root_mean_squared_error: 0.7105 - val_loss: 0.4763 - val_root_mean_squared_error: 0.6902\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.5034 - root_mean_squared_error: 0.7095 - val_loss: 0.4742 - val_root_mean_squared_error: 0.6886\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 0.5012 - root_mean_squared_error: 0.7080 - val_loss: 0.4730 - val_root_mean_squared_error: 0.6877\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.4991 - root_mean_squared_error: 0.7064 - val_loss: 0.4711 - val_root_mean_squared_error: 0.6864\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.4967 - root_mean_squared_error: 0.7048 - val_loss: 0.4701 - val_root_mean_squared_error: 0.6857\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.4967 - root_mean_squared_error: 0.7048 - val_loss: 0.4693 - val_root_mean_squared_error: 0.6850\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.4954 - root_mean_squared_error: 0.7039 - val_loss: 0.4684 - val_root_mean_squared_error: 0.6844\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 0.4941 - root_mean_squared_error: 0.7029 - val_loss: 0.4679 - val_root_mean_squared_error: 0.6840\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.4942 - root_mean_squared_error: 0.7030 - val_loss: 0.4675 - val_root_mean_squared_error: 0.6837\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 2s 131ms/step - loss: 0.4930 - root_mean_squared_error: 0.7022 - val_loss: 0.4667 - val_root_mean_squared_error: 0.6832\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 0.4915 - root_mean_squared_error: 0.7011 - val_loss: 0.4666 - val_root_mean_squared_error: 0.6831\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.4913 - root_mean_squared_error: 0.7009 - val_loss: 0.4663 - val_root_mean_squared_error: 0.6829\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 2s 122ms/step - loss: 0.4909 - root_mean_squared_error: 0.7006 - val_loss: 0.4661 - val_root_mean_squared_error: 0.6827\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.4890 - root_mean_squared_error: 0.6993 - val_loss: 0.4658 - val_root_mean_squared_error: 0.6825\n",
      "1875/1875 [==============================] - 11s 5ms/step - loss: 0.4730 - root_mean_squared_error: 0.6877\n",
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_130 (Dense)           (None, 200)               400       \n",
      "                                                                 \n",
      " batch_normalization_78 (Ba  (None, 200)               800       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_127 (Activation  (None, 200)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_77 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " dense_131 (Dense)           (None, 200)               40200     \n",
      "                                                                 \n",
      " batch_normalization_79 (Ba  (None, 200)               800       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_128 (Activation  (None, 200)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_78 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " dense_132 (Dense)           (None, 200)               40200     \n",
      "                                                                 \n",
      " batch_normalization_80 (Ba  (None, 200)               800       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_129 (Activation  (None, 200)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_79 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " dense_133 (Dense)           (None, 1)                 201       \n",
      "                                                                 \n",
      " activation_130 (Activation  (None, 1)                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 83401 (325.79 KB)\n",
      "Trainable params: 82201 (321.10 KB)\n",
      "Non-trainable params: 1200 (4.69 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 3 hidden layers\n",
    "\n",
    "rmse_3 = []\n",
    "\n",
    "for i, value in enumerate(units_list):\n",
    "    \n",
    "    # train the nn model on training set\n",
    "    model = creat_model_3_layer(units_list[i]) \n",
    "    model.fit(X_train, y_train, batch_size = 3000, epochs=30, validation_split=0.2)\n",
    "    \n",
    "    # get prediction from testing set\n",
    "    score = model.evaluate(X_test, y_test)\n",
    "    rmse_3.append(score[1]) # score[0] is the loss\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2714a3c1",
   "metadata": {},
   "source": [
    "## Number of parameters learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5fd805",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_1 = [141, 281, 421, 561, 701, 841, 981, 1121, 1261, 1401]\n",
    "params_2 = [641, 2081, 4321, 7361, 11201, 15841, 21281, 27521, 34561, 42401]\n",
    "params_3 = [1141, 3881, 8221, 14161, 21701, 30841, 41581, 53921, 67861, 83401]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b49209a",
   "metadata": {},
   "source": [
    "## Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "91005afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(units_list, rmse_1, 'r', label='1 hidden')\n",
    "plt.plot(units_list, rmse_2, 'g', label='2 hidden')\n",
    "plt.plot(units_list, rmse_3, 'b', label='3 hidden')\n",
    "# ax1.title('Training and Validation RMSE')\n",
    "plt.xlabel('Number of units')\n",
    "plt.ylabel('Test error (RMSE)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(params_1, rmse_1, 'r', label='1 hidden')\n",
    "plt.plot(params_2, rmse_2, 'g', label='2 hidden')\n",
    "plt.plot(params_3, rmse_3, 'b', label='3 hidden')\n",
    "# plt.title('Training and Validation RMSE')\n",
    "plt.xlabel('Number of parameters')\n",
    "# plt.ylabel('Test error (RMSE)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3018cf",
   "metadata": {},
   "source": [
    "## Summary\n",
    "As shown, we are able to observe the performance difference among the 3 models.   \n",
    "The 3 layers version performs much better to generate lower RMSE than other 2 shallow versions as the number of units increase.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
